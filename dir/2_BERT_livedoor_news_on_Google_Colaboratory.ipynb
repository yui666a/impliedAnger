{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VYlVm0kpjx57"
   },
   "source": [
    "## 日本語BERTでlivedoorニュースを教師あり学習で分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IqIW6ar2Bm3J",
    "outputId": "44476f53-a1fc-4f50-e953-50441bef9c21",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f68b5c0afd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 乱数シードの固定\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "SEED_VALUE = 1234  # これはなんでも良い\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
    "random.seed(SEED_VALUE)\n",
    "np.random.seed(SEED_VALUE)\n",
    "torch.manual_seed(SEED_VALUE)  # PyTorchを使う場合\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ii-mqaAhCApI"
   },
   "source": [
    "### GPUの使用可能を確認\n",
    "\n",
    "画面上部のメニュー ランタイム > ランタイムのタイプを変更 で、 ノートブックの設定 を開く\n",
    "\n",
    "ハードウェアアクセラレータに GPU を選択し、 保存 する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W8TJgawCB_Nb",
    "outputId": "81b1d32b-d3f3-494b-f414-dd3d26c9c4b6",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPUの使用確認：True or False\n",
    "torch.cuda.is_available()\n",
    "\n",
    "# TrueならGPU使用可能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gc6dbVUfj1W-"
   },
   "source": [
    "## 準備1：Livedoorニュースをダウンロードしてtsvファイル化\n",
    "\n",
    "参考：https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/finetune-to-livedoor-corpus.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209
    },
    "colab_type": "code",
    "id": "8Vh0a49Gp-rS",
    "outputId": "1b327511-b985-44fa-899f-a23c308ffa6c",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-02-06 15:15:14--  https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
      "Resolving www.rondhuit.com (www.rondhuit.com)... 59.106.19.174\n",
      "Connecting to www.rondhuit.com (www.rondhuit.com)|59.106.19.174|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8855190 (8.4M) [application/x-gzip]\n",
      "Saving to: ‘ldcc-20140209.tar.gz’\n",
      "\n",
      "ldcc-20140209.tar.g 100%[===================>]   8.44M  24.5MB/s    in 0.3s    \n",
      "\n",
      "2023-02-06 15:15:15 (24.5 MB/s) - ‘ldcc-20140209.tar.gz’ saved [8855190/8855190]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Livedoorニュースのファイルをダウンロード\n",
    "! wget \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "keY2WGdwjzLD",
    "outputId": "e62b13c3-aa74-4b49-cc7b-7cc36cf08e73",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHANGES.txt', 'dokujo-tsushin', 'it-life-hack', 'kaden-channel', 'livedoor-homme', 'movie-enter', 'peachy', 'README.txt', 'smax', 'sports-watch', 'topic-news']\n",
      "カテゴリー数: 9\n",
      "['dokujo-tsushin', 'it-life-hack', 'kaden-channel', 'livedoor-homme', 'movie-enter', 'peachy', 'smax', 'sports-watch', 'topic-news']\n"
     ]
    }
   ],
   "source": [
    "# ファイルを解凍し、カテゴリー数と内容を確認\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# 解凍\n",
    "tar = tarfile.open(\"ldcc-20140209.tar.gz\", \"r:gz\")\n",
    "tar.extractall(\"./data/livedoor/\")\n",
    "tar.close()\n",
    "\n",
    "# フォルダのファイルとディレクトリを確認\n",
    "files_folders = [name for name in os.listdir(\"./data/livedoor/text/\")]\n",
    "print(files_folders)\n",
    "\n",
    "# カテゴリーのフォルダのみを抽出\n",
    "categories = [name for name in os.listdir(\n",
    "    \"./data/livedoor/text/\") if os.path.isdir(\"./data/livedoor/text/\"+name)]\n",
    "\n",
    "print(\"カテゴリー数:\", len(categories))\n",
    "print(categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "Z201OQ7gvYOY",
    "outputId": "9766d63d-029f-4a2c-81b1-accb8ad0dffd",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0： http://news.livedoor.com/article/detail/6255260/\n",
      "\n",
      "1： 2012-02-07T09:00:00+0900\n",
      "\n",
      "2： 新しいヴァンパイアが誕生！　ジョニデ主演『ダーク・シャドウ』の公開日が決定\n",
      "\n",
      "3： 　こんなヴァンパイアは見たことがない！　ジョニー・デップとティム・バートン監督がタッグを組んだ映画『ダーク・シャドウズ（原題）』の邦題が『ダーク・シャドウ』に決定。日本公開日が5月19日に決まった。さらに、ジョニー・デップ演じるヴァンパイアの写真が公開された。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ファイルの中身を確認してみる\n",
    "file_name = \"./data/livedoor/text/movie-enter/movie-enter-6255260.txt\"\n",
    "\n",
    "with open(file_name) as text_file:\n",
    "    text = text_file.readlines()\n",
    "    print(\"0：\", text[0])  # URL情報\n",
    "    print(\"1：\", text[1])  # タイムスタンプ\n",
    "    print(\"2：\", text[2])  # タイトル\n",
    "    print(\"3：\", text[3])  # 本文\n",
    "\n",
    "    # 今回は4要素目には本文は伸びていないが、4要素目以降に本文がある場合もある\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CoKvaAK1vurV",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 本文を取得する前処理関数を定義\n",
    "\n",
    "\n",
    "def extract_main_txt(file_name):\n",
    "    with open(file_name) as text_file:\n",
    "        # 今回はタイトル行は外したいので、3要素目以降の本文のみ使用\n",
    "        text = text_file.readlines()[3:]\n",
    "\n",
    "        # 3要素目以降にも本文が入っている場合があるので、リストにして、後で結合させる\n",
    "        text = [sentence.strip() for sentence in text]  # 空白文字(スペースやタブ、改行)の削除\n",
    "        text = list(filter(lambda line: line != '', text))\n",
    "        text = ''.join(text)\n",
    "        text = text.translate(str.maketrans(\n",
    "            {'\\n': '', '\\t': '', '\\r': '', '\\u3000': ''}))  # 改行やタブ、全角スペースを消す\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vq2ebKoOxThi",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# リストに前処理した本文と、カテゴリーのラベルを追加していく\n",
    "import glob\n",
    "\n",
    "list_text = []\n",
    "list_label = []\n",
    "\n",
    "for cat in categories:\n",
    "    text_files = glob.glob(os.path.join(\"./data/livedoor/text\", cat, \"*.txt\"))\n",
    "\n",
    "    # 前処理extract_main_txtを実施して本文を取得\n",
    "    body = [extract_main_txt(text_file) for text_file in text_files]\n",
    "\n",
    "    label = [cat] * len(body)  # bodyの数文だけカテゴリー名のラベルのリストを作成\n",
    "\n",
    "    list_text.extend(body)  # appendが要素を追加するのに対して、extendはリストごと追加する\n",
    "    list_label.extend(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "bKPb_LJxxuOM",
    "outputId": "51a0fb36-5377-4f79-8672-f89f8d88aa51",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "もうすぐジューン・ブライドと呼ばれる６月。独女の中には自分の式はまだなのに呼ばれてばかり……という「お祝い貧乏」状態の人も多いのではないだろうか？さらに出席回数を重ねていくと、こんなお願いごとをされることも少なくない。「お願いがあるんだけど……友人代表のスピーチ、やってくれないかな？」さてそんなとき、独女はどう対応したらいいか？最近だとインターネット等で検索すれば友人代表スピーチ用の例文サイトがたくさん出てくるので、それらを参考にすれば、無難なものは誰でも作成できる。しかし由利さん（33歳）はネットを参考にして作成したものの「これで本当にいいのか不安でした。一人暮らしなので聞かせて感想をいってくれる人もいないし、かといって他の友人にわざわざ聞かせるのもどうかと思うし……」ということで活用したのが、なんとインターネットの悩み相談サイトに。そこに作成したスピーチ文を掲載し「これで大丈夫か添削してください」とメッセージを送ったというのである。「一晩で3人位の人が添削してくれましたよ。ちなみに自分以外にもそういう人はたくさんいて、その相談サイトには同じように添削をお願いする投稿がいっぱいありました」（由利さん）。ためしに教えてもらったそのサイトをみてみると、確かに「結婚式のスピーチの添削お願いします」という投稿が1000件を超えるくらいあった。めでたい結婚式の影でこんなネットコミュニティがあったとは知らなかった。しかし「事前にお願いされるスピーチなら準備ができるしまだいいですよ。一番嫌なのは何といってもサプライズスピーチ！」と語るのは昨年だけで10万以上お祝いにかかったというお祝い貧乏独女の薫さん（35歳）「私は基本的に人前で話すのが苦手なんですよ。だからいきなり指名されるとしどろもどろになって何もいえなくなる。そうすると自己嫌悪に陥って終わった後でもまったく楽しめなくなりますね」サプライズスピーチのメリットとしては、準備していない状態なので、フランクな本音をしゃべってもらえるという楽しさがあるようだ。しかしそれも上手に対応できる人ならいいが、苦手な人の場合だと「フランク」ではなく「しどろもどろ」になる危険性大。ちなみにプロの司会者の場合、本当のサプライズではなく式の最中に「のちほどサプライズスピーチとしてご指名させていただきます」という一言があることも多いようだが、薫さん曰く「そんな何分前に言われても無理！」らしい。要は「サプライズを楽しめる」というタイプの人選が大切ということか。一方「ありきたりじゃつまらないし、ネットで例文を検索している際に『こんな方法もあるのか！』って思って取り入れました」という幸恵さん（30歳）が行ったスピーチは「手紙形式のスピーチ」というもの。「○○ちゃんへみたいな感じで新婦の友人にお手紙を書いて読み上げるやり方です。これなら多少フランクな書き方でも大丈夫だし、何より暗記しないで堂々と読み上げることができますよね。読んだものはそのまま友人にあげれば一応記念にもなります」（幸恵さん）なるほど、確かにこれなら読みあげればいいだけなので、人前で話すのが苦手な人でも失敗しないかもしれない。主役はあくまで新郎新婦ながらも、いざとなると緊張し、内容もあれこれ考えて、こっそりリハーサル……そんな人知れず頑張るスピーチ担当独女たちにも幸あれ（高山惠）\n",
      "dokujo-tsushin\n"
     ]
    }
   ],
   "source": [
    "# 0番目の文章とラベルを確認\n",
    "print(list_text[0])\n",
    "print(list_label[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "a_Kf1D9xxuvP",
    "outputId": "f70a9ee6-28e7-40c6-8649-4ee921244805",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7376, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>もうすぐジューン・ブライドと呼ばれる６月。独女の中には自分の式はまだなのに呼ばれてばかり……...</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>携帯電話が普及する以前、恋人への連絡ツールは一般電話が普通だった。恋人と別れたら、手帳に書か...</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>「男性はやっぱり、女性の“すっぴん”が大好きなんですかね」と不満そうに話すのは、出版関係で働...</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ヒップの加齢による変化は「たわむ→下がる→内に流れる」、バストは「そげる→たわむ→外に流れる...</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6月から支給される子ども手当だが、当初は子ども一人当たり月額2万6000円が支給されるはずだ...</td>\n",
       "      <td>dokujo-tsushin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text           label\n",
       "0  もうすぐジューン・ブライドと呼ばれる６月。独女の中には自分の式はまだなのに呼ばれてばかり……...  dokujo-tsushin\n",
       "1  携帯電話が普及する以前、恋人への連絡ツールは一般電話が普通だった。恋人と別れたら、手帳に書か...  dokujo-tsushin\n",
       "2  「男性はやっぱり、女性の“すっぴん”が大好きなんですかね」と不満そうに話すのは、出版関係で働...  dokujo-tsushin\n",
       "3  ヒップの加齢による変化は「たわむ→下がる→内に流れる」、バストは「そげる→たわむ→外に流れる...  dokujo-tsushin\n",
       "4  6月から支給される子ども手当だが、当初は子ども一人当たり月額2万6000円が支給されるはずだ...  dokujo-tsushin"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pandasのDataFrameにする\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'text': list_text, 'label': list_label})\n",
    "\n",
    "# 大きさを確認しておく（7,376文章が存在）\n",
    "print(df.shape)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "kB8p83xi02ck",
    "outputId": "cdf990df-cf20-474e-8f40-6bdef266d772",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'dokujo-tsushin', 1: 'it-life-hack', 2: 'kaden-channel', 3: 'livedoor-homme', 4: 'movie-enter', 5: 'peachy', 6: 'smax', 7: 'sports-watch', 8: 'topic-news'}\n",
      "{'dokujo-tsushin': 0, 'it-life-hack': 1, 'kaden-channel': 2, 'livedoor-homme': 3, 'movie-enter': 4, 'peachy': 5, 'smax': 6, 'sports-watch': 7, 'topic-news': 8}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>もうすぐジューン・ブライドと呼ばれる６月。独女の中には自分の式はまだなのに呼ばれてばかり……...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>携帯電話が普及する以前、恋人への連絡ツールは一般電話が普通だった。恋人と別れたら、手帳に書か...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>「男性はやっぱり、女性の“すっぴん”が大好きなんですかね」と不満そうに話すのは、出版関係で働...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ヒップの加齢による変化は「たわむ→下がる→内に流れる」、バストは「そげる→たわむ→外に流れる...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6月から支給される子ども手当だが、当初は子ども一人当たり月額2万6000円が支給されるはずだ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label_index\n",
       "0  もうすぐジューン・ブライドと呼ばれる６月。独女の中には自分の式はまだなのに呼ばれてばかり……...            0\n",
       "1  携帯電話が普及する以前、恋人への連絡ツールは一般電話が普通だった。恋人と別れたら、手帳に書か...            0\n",
       "2  「男性はやっぱり、女性の“すっぴん”が大好きなんですかね」と不満そうに話すのは、出版関係で働...            0\n",
       "3  ヒップの加齢による変化は「たわむ→下がる→内に流れる」、バストは「そげる→たわむ→外に流れる...            0\n",
       "4  6月から支給される子ども手当だが、当初は子ども一人当たり月額2万6000円が支給されるはずだ...            0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# カテゴリーの辞書を作成\n",
    "dic_id2cat = dict(zip(list(range(len(categories))), categories))\n",
    "dic_cat2id = dict(zip(categories, list(range(len(categories)))))\n",
    "\n",
    "print(dic_id2cat)\n",
    "print(dic_cat2id)\n",
    "\n",
    "# DataFrameにカテゴリーindexの列を作成\n",
    "df[\"label_index\"] = df[\"label\"].map(dic_cat2id)\n",
    "df.head()\n",
    "\n",
    "# label列を消去し、text, indexの順番にする\n",
    "df = df.loc[:, [\"text\", \"label_index\"]]\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "zaE_8vER18xY",
    "outputId": "189432c8-db85-488d-a616-b4c00f97eb71",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Peachyでも大人気の「恋愛」をテーマにした記事の週間ランキングです！2012年6月21日...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>値下げや利用可能国が拡大！テレコムスクエアは1日、ポストペイドタイプの海外向けSIMカード「...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>４月。新しい年度を迎え、新しい顧客の開拓、新しい取引先、新しい上司や同僚。エネルギーに満ち溢...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xperia GXが発表！ソニーモバイルコミュニケーションズは9日、国内向けとして2012年...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1月も後半に入り、バレンタインデーの話題も増えてきた今日この頃。皆さんはチョコを贈る相手、決...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label_index\n",
       "0  Peachyでも大人気の「恋愛」をテーマにした記事の週間ランキングです！2012年6月21日...            5\n",
       "1  値下げや利用可能国が拡大！テレコムスクエアは1日、ポストペイドタイプの海外向けSIMカード「...            6\n",
       "2  ４月。新しい年度を迎え、新しい顧客の開拓、新しい取引先、新しい上司や同僚。エネルギーに満ち溢...            3\n",
       "3  Xperia GXが発表！ソニーモバイルコミュニケーションズは9日、国内向けとして2012年...            6\n",
       "4  1月も後半に入り、バレンタインデーの話題も増えてきた今日この頃。皆さんはチョコを贈る相手、決...            5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 順番をシャッフルする\n",
    "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "TRrt-XH72C3l",
    "outputId": "344d24a3-4b2d-4a7b-c9e2-daa538645b55",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1475, 2)\n",
      "(5901, 2)\n"
     ]
    }
   ],
   "source": [
    "# tsvファイルで保存する\n",
    "\n",
    "# 全体の2割の文章数\n",
    "len_0_2 = len(df) // 5\n",
    "\n",
    "# 前から2割をテストデータとする\n",
    "df[:len_0_2].to_csv(\"./test.tsv\", sep='\\t', index=False, header=None)\n",
    "print(df[:len_0_2].shape)\n",
    "\n",
    "# 前2割からを訓練&検証データとする\n",
    "df[len_0_2:].to_csv(\"./train_eval.tsv\", sep='\\t', index=False, header=None)\n",
    "print(df[len_0_2:].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0NykW7u3Mw9",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# tsvファイルをダウンロードしたい場合\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m files\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ダウンロードする場合はコメントを外す\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 少し時間がかかる（4MB）\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# files.download(\"./test.tsv\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 少し時間がかかる（18MB）\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# files.download(\"./train_eval.tsv\")\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "# tsvファイルをダウンロードしたい場合\n",
    "from google.colab import files\n",
    "\n",
    "# ダウンロードする場合はコメントを外す\n",
    "# 少し時間がかかる（4MB）\n",
    "# files.download(\"./test.tsv\")\n",
    "\n",
    "\n",
    "# ダウンロードする場合はコメントを外す\n",
    "# 少し時間がかかる（18MB）\n",
    "# files.download(\"./train_eval.tsv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPT3Pjr94oPW"
   },
   "source": [
    "## 準備2：LivedoorニュースをBERT用のDataLoaderにする\n",
    "\n",
    "Hugginfaceのリポジトリの案内とは異なり、torchtextを使用した手法で実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "colab_type": "code",
    "id": "kPXX4pr2-kY-",
    "outputId": "2a7d359e-0edd-4591-b13a-27e16ce8a075",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mE: \u001b[0mCould not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\u001b[0m\n",
      "\u001b[1;31mE: \u001b[0mUnable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\u001b[0m\n",
      "/usr/bin/sh: 1: aptitude: not found\n",
      "Collecting mecab-python3\n",
      "  Using cached mecab_python3-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (577 kB)\n",
      "Installing collected packages: mecab-python3\n",
      "Successfully installed mecab-python3-1.0.6\n",
      "Collecting transformers==2.9.0\n",
      "  Downloading transformers-2.9.0-py3-none-any.whl (635 kB)\n",
      "\u001b[K     |████████████████████████████████| 635 kB 8.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from transformers==2.9.0) (2.28.2)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0-cp38-cp38-manylinux1_x86_64.whl (7.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.5 MB 70.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in ./.local/lib/python3.8/site-packages (from transformers==2.9.0) (0.1.97)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (from transformers==2.9.0) (1.24.1)\n",
      "Processing ./.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4/sacremoses-0.0.53-py3-none-any.whl\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.8/site-packages (from transformers==2.9.0) (3.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.8/site-packages (from transformers==2.9.0) (2022.10.31)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.8/site-packages (from transformers==2.9.0) (4.64.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.8/site-packages (from requests->transformers==2.9.0) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.8/site-packages (from requests->transformers==2.9.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./.local/lib/python3.8/site-packages (from requests->transformers==2.9.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.8/site-packages (from requests->transformers==2.9.0) (2022.12.7)\n",
      "Requirement already satisfied: click in ./.local/lib/python3.8/site-packages (from sacremoses->transformers==2.9.0) (8.1.3)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers==2.9.0) (1.14.0)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.8/site-packages (from sacremoses->transformers==2.9.0) (1.2.0)\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.2\n",
      "    Uninstalling tokenizers-0.13.2:\n",
      "      Successfully uninstalled tokenizers-0.13.2\n",
      "\u001b[31mERROR: Could not install packages due to an EnvironmentError: [Errno 16] Device or resource busy: '.nfs0000000002d42bf70000033d'\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# MeCabとtransformersの用意\n",
    "!apt install aptitude swig\n",
    "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
    "!pip install mecab-python3\n",
    "!pip install transformers==2.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3tKqo2TF9vzj",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m  \u001b[38;5;66;03m# torchtextを使用\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_bert\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_bert_japanese\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertJapaneseTokenizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext  # torchtextを使用\n",
    "from transformers.modeling_bert import BertModel\n",
    "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
    "\n",
    "# 日本語BERTの分かち書き用tokenizerです\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
    "    'bert-base-japanese-whole-word-masking')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0ZypBWaE-PB6",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchtext' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"torchtextのtokenizerとして扱えるように、512単語のpytorchでのencodeを定義。ここで[0]を指定し忘れないように\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 11\u001b[0m TEXT \u001b[38;5;241m=\u001b[39m \u001b[43mtorchtext\u001b[49m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mField(sequential\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, tokenize\u001b[38;5;241m=\u001b[39mtokenizer_512, use_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m                             include_lengths\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fix_length\u001b[38;5;241m=\u001b[39mmax_length, pad_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 注意：tokenize=tokenizer.encodeと、.encodeをつけます。padding[PAD]のindexが0なので、0を指定します。\u001b[39;00m\n\u001b[1;32m     15\u001b[0m LABEL \u001b[38;5;241m=\u001b[39m torchtext\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mField(sequential\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_vocab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchtext' is not defined"
     ]
    }
   ],
   "source": [
    "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "\n",
    "max_length = 512  # 東北大学_日本語版の最大の単語数（サブワード数）は512\n",
    "\n",
    "\n",
    "def tokenizer_512(input_text):\n",
    "    \"\"\"torchtextのtokenizerとして扱えるように、512単語のpytorchでのencodeを定義。ここで[0]を指定し忘れないように\"\"\"\n",
    "    return tokenizer.encode(input_text, max_length=512, return_tensors='pt')[0]\n",
    "\n",
    "\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_512, use_vocab=False, lower=False,\n",
    "                            include_lengths=True, batch_first=True, fix_length=max_length, pad_token=0)\n",
    "# 注意：tokenize=tokenizer.encodeと、.encodeをつけます。padding[PAD]のindexが0なので、0を指定します。\n",
    "\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "# (注釈)：各引数を再確認\n",
    "# sequential: データの長さが可変か？文章は長さがいろいろなのでTrue.ラベルはFalse\n",
    "# tokenize: 文章を読み込んだときに、前処理や単語分割をするための関数を定義\n",
    "# use_vocab：単語をボキャブラリーに追加するかどうか\n",
    "# lower：アルファベットがあったときに小文字に変換するかどうか\n",
    "# include_length: 文章の単語数のデータを保持するか\n",
    "# batch_first：ミニバッチの次元を用意するかどうか\n",
    "# fix_length：全部の文章をfix_lengthと同じ長さになるように、paddingします\n",
    "# init_token, eos_token, pad_token, unk_token：文頭、文末、padding、未知語に対して、どんな単語を与えるかを指定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jyLNL-sd_Xd5",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 各tsvファイルを読み込み、分かち書きをしてdatasetにします\n",
    "# 少し時間がかかります\n",
    "# train_eval：5901個、test：1475個\n",
    "dataset_train_eval, dataset_test = torchtext.data.TabularDataset.splits(\n",
    "    path='.', train='train_eval.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "aNRofv_iDOYq",
    "outputId": "c4a58711-2136-4466-90b1-76b9241d2d42",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4426\n",
      "1475\n",
      "1475\n"
     ]
    }
   ],
   "source": [
    "# torchtext.data.Datasetのsplit関数で訓練データと検証データを分ける\n",
    "# train_eval：5901個、test：1475個\n",
    "\n",
    "dataset_train, dataset_eval = dataset_train_eval.split(\n",
    "    split_ratio=1.0 - 1475/5901, random_state=random.seed(1234))\n",
    "\n",
    "# datasetの長さを確認してみる\n",
    "print(dataset_train.__len__())\n",
    "print(dataset_eval.__len__())\n",
    "print(dataset_test.__len__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 959
    },
    "colab_type": "code",
    "id": "O_XilciAI2la",
    "outputId": "8a3d360a-a6aa-4bae-e653-e2863bf6e690",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    2,  8454,  6172,  1704,  5408,    16,  2109,    20,    16,    33,\n",
      "           29,     6,   802, 28841, 28798,  2935,  8722,    40,   580,    26,\n",
      "           20,    16,    33,  8454,     7,     9,     6,  3799,    18, 20093,\n",
      "           14,    31,  4895,     8,   218,    11,  4021,    16,  6629,     5,\n",
      "           28,     6,  8454,    11,  3062, 20206, 15591,     5,  1628,  6172,\n",
      "          120,     8, 21966,     6,    36,  6740,     5,   745,   536, 11418,\n",
      "           38,    13, 15414,   205,     8,  1216,  8454,  1178,  9494,     5,\n",
      "           74,    35,  2154,  3111,    10,     5,     9,     6,  2154,    94,\n",
      "        13493,   280,     7,    31,    36, 20498,  2154, 25239, 28578,  4815,\n",
      "           38,     8,  1173, 28532, 28450,    91,   587,  8454,  2580,     5,\n",
      "         1075,     9,     6,  2154,    12,  2857,    10,    13,     5,    45,\n",
      "            8,  2381,    11,  5003,     6, 23429,   371,  9542,  1851,     6,\n",
      "         1162,  1134,    29,     8,    26, 28468, 17985,  1851,     5,    51,\n",
      "            7,  1861,    11,  7831,  6715,    13,     6, 12272, 16559,     5,\n",
      "         3208,    14,    73, 27090, 28547,     8,   205, 31958,     6,  1757,\n",
      "           13, 14355,    14,  4611, 28468,    16,  1497,     8,    26, 28456,\n",
      "            6,  8454,     5,  8112,     9,  1499,  2973,    16,    48,   181,\n",
      "            8, 28383,    42, 30348,     5, 12039, 30481,     6, 15437,     6,\n",
      "          893,   326,     8,  5501, 15437,     7,  1610,    62,   434, 16233,\n",
      "          140,  6049,    14,     6,  8454,  5690,     5,  2155, 28614,    13,\n",
      "        12891,    11, 10057,    16,    33,     5,    75,     8,   521,     6,\n",
      "        20498,    12,     9,  1469,     6,  6291,     6,  2672,    64,   324,\n",
      "            5, 22487,    18, 12039, 30481,    11,     6, 15437,     9,  5354,\n",
      "          997,     5, 20915, 28483, 25224,    64,    11,   406,    15,    16,\n",
      "           33,     8,  3188,   326,    13, 12039, 30481,     6,   893, 11853,\n",
      "         1123, 28555,    64,     5,  1367,  8112,    11,  1200,  7294,    12,\n",
      "         9698,  1396,     6, 12039, 30481,     7,  1610,    62,  7249, 30221,\n",
      "        28575,    11,  6824, 28640,    26,   191,    16,     6, 12039, 16503,\n",
      "           11, 18595,     8,  1778,  4422,   312,  1851,   186,     7,     9,\n",
      "         1681,  5558, 28504,    28,  1753, 12798, 28457,     6, 14072, 28470,\n",
      "         9962,    18, 23071,    14, 10385, 28468,    16,    33,     8,   171,\n",
      "            9,  8112,    12,    31,    42, 30348,    11,  9698,  2078,    10,\n",
      "           45,    12,  2824, 12891,    75,    13,    29,     8,    70, 12039,\n",
      "        16503,    11, 20498,     5,  2582,  2442,    12,    28,    31,    36,\n",
      "         4749, 18767, 28477,    38,    13,  2575,     6, 12039, 16503,    11,\n",
      "         8296,  3952,    10,    83,     7, 17213,   326,    11,  8326,    16,\n",
      "         9698,  1396,    16,  3379,    10, 12039, 16503,    11,    36,   287,\n",
      "          246, 18767, 28477,    38,    13,   625,     8,   366,     6, 12039,\n",
      "        16503,     7, 15437,    11,  1334,     6,  7968,    83,     6, 13119,\n",
      "         6641,   118,    13,  8234,    62,     8, 13119,  6641,    12,     9,\n",
      "        12039, 16503,     7, 21859,    11,  1334,    16,     6, 13119,    26,\n",
      "          796,     8,   171,    14,  1113,  1279,    18,  9621,    12,    31,\n",
      "            8, 21859,    14, 12039, 16503,     7,  1610,    62,  6824, 28593,\n",
      "           11,  5170,    15,     6,  7659,    13, 14458,  2988,    11, 18595,\n",
      "            8,  3686,     6, 21859,   230, 13119,    26,   191,    80,    13,\n",
      "            6,  8454,     7,   737,    80,   140,    45,     8, 13119,     9,\n",
      "           57,    32,    12,  9395,     7,  4017,     6,   127,    32,    12,\n",
      "         1122,     6,    36,  1099,  8454,    38,   140,   120,     7,   139,\n",
      "            8,   288,     9, 27335,     8, 25215, 30753, 28965,    64,    12,\n",
      "         1099,  8454,     7,  1610,    62, 21859,    49, 16816,    18,  6865,\n",
      "           11,  8549,     8,   171,   225,  5934,    12,  6297,  7041,  1232,\n",
      "            5,     3])\n",
      "長さ： 512\n",
      "ラベル： 7\n"
     ]
    }
   ],
   "source": [
    "# datasetの中身を確認してみる\n",
    "item = next(iter(dataset_train))\n",
    "print(item.Text)\n",
    "print(\"長さ：\", len(item.Text))  # 長さを確認 [CLS]から始まり[SEP]で終わる。512より長いと後ろが切れる\n",
    "print(\"ラベル：\", item.Label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "j7323f5aJFzz",
    "outputId": "709bd9d4-b4b5-47c7-d2a6-2b994085a861",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ビール', 'って', 'どう', 'やっ', 'て', '作ら', 'れ', 'て', 'いる', 'か', '、', 'ご', '##存', '##知', '?', '各社', 'から', '発売', 'さ', 'れ', 'て', 'いる', 'ビール', 'に', 'は', '、', 'さまざま', 'な', 'こだわり', 'が', 'ある', '模様', '。', 'それ', 'を', '知っ', 'て', 'おく', 'の', 'も', '、', 'ビール', 'を', '味', '##わう', '楽しみ', 'の', 'ひと', 'って', 'もの', '。', '早速', '、', '「', '大人', 'の', '社会', '科', '見学', '」', 'と', 'いこ', 'う', '。', '国内', 'ビール', '生産', '発祥', 'の', '地', '・', '横浜', '訪れ', 'た', 'の', 'は', '、', '横浜', '市', '鶴見', '区', 'に', 'ある', '「', 'キリン', '横浜', 'ビア', '##ビ', '##レッジ', '」', '。', 'じ', '##つ', '##は', '日本', 'における', 'ビール', '作り', 'の', '歴史', 'は', '、', '横浜', 'で', '始まっ', 'た', 'と', 'の', 'こと', '。', '伝統', 'を', '守る', '、', '由緒', '正', '##しき', '工場', '、', 'といった', 'ところ', 'か', '。', 'さ', '##っ', '##そく', '工場', 'の', '中', 'に', '脚', 'を', '踏み', '##入れる', 'と', '、', 'こんな', '泡', 'の', '演出', 'が', 'お', '出迎', '##え', '。', 'う', '##ぅ', '、', '自然', 'と', '喉', 'が', '鳴', '##っ', 'て', 'しまう', '。', 'さ', '##て', '、', 'ビール', 'の', '原料', 'は', '大きく', '分け', 'て', '3', 'つ', '。', '二条', '大', '##麦', 'の', '麦', '##芽', '、', 'ホップ', '、', 'そして', '水', '。', 'ちなみに', 'ホップ', 'に', '含ま', 'れる', 'ル', '##プリン', 'という', '成分', 'が', '、', 'ビール', '独特', 'の', '苦', '##み', 'と', '香り', 'を', '生み出し', 'て', 'いる', 'の', 'だ', '。', 'なお', '、', 'キリン', 'で', 'は', 'ヨーロッパ', '、', '北米', '、', 'オーストラリア', 'など', '世界', 'の', '良質', 'な', '麦', '##芽', 'を', '、', 'ホップ', 'は', 'チェコ', '産', 'の', 'ファイン', '##ア', '##ロマ', 'など', 'を', '使用', 'し', 'て', 'いる', '。', 'まず', '水', 'と', '麦', '##芽', '、', 'そして', 'コーン', '##スター', '##チ', 'など', 'の', '副', '原料', 'を', '大きな', '釜', 'で', '煮', '##出し', '、', '麦', '##芽', 'に', '含ま', 'れる', 'でん', '##ぷ', '##ん', 'を', '糖', '##化', 'さ', 'せ', 'て', '、', '麦', '汁', 'を', '作り出す', '。', 'そう', 'いえ', 'ば', '工場', '内', 'に', 'は', '香', '##ばし', '##く', 'も', '酸', '##っぱ', '##い', '、', 'なに', '##か', '不思議', 'な', '匂い', 'が', '漂', '##っ', 'て', 'いる', '。', 'これ', 'は', '原料', 'で', 'ある', '大', '##麦', 'を', '煮', '出し', 'た', 'こと', 'で', '生まれる', '香り', 'だ', 'と', 'か', '。', 'この', '麦', '汁', 'を', 'キリン', 'の', 'メイン', '商品', 'で', 'も', 'ある', '「', '一番', '搾', '##り', '」', 'と', 'いい', '、', '麦', '汁', 'を', '一度', 'とっ', 'た', '後', 'に', 'もう一度', '水', 'を', 'いれ', 'て', '煮', '##出し', 'て', '作っ', 'た', '麦', '汁', 'を', '「', '二', '番', '搾', '##り', '」', 'と', 'いう', '。', 'その後', '、', '麦', '汁', 'に', 'ホップ', 'を', '加え', '、', '冷却', '後', '、', '発酵', 'タンク', 'へ', 'と', '移さ', 'れる', '。', '発酵', 'タンク', 'で', 'は', '麦', '汁', 'に', '酵母', 'を', '加え', 'て', '、', '発酵', 'さ', 'せる', '。', 'これ', 'が', '最も', '重要', 'な', '工程', 'で', 'ある', '。', '酵母', 'が', '麦', '汁', 'に', '含ま', 'れる', '糖', '##分', 'を', '分解', 'し', '、', 'アルコール', 'と', '炭酸', 'ガス', 'を', '作り出す', '。', 'つまり', '、', '酵母', 'によって', '発酵', 'さ', 'せ', 'ない', 'と', '、', 'ビール', 'に', 'なら', 'ない', 'という', 'こと', '。', '発酵', 'は', '4', '日', 'で', 'ピーク', 'に', '達し', '、', '7', '日', 'で', '終了', '、', '「', '若', 'ビール', '」', 'という', 'もの', 'に', 'なる', '。', '次', 'は', '濾過', '。', '珪', '##藻', '##土', 'など', 'で', '若', 'ビール', 'に', '含ま', 'れる', '酵母', 'や', '無駄', 'な', 'タンパク質', 'を', '除去', '。', 'これ', 'により', 'クリア', 'で', '美しい', '黄金', '色', 'の', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'livedoor-homme'"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datasetの中身を文章に戻し、確認\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(item.Text.tolist()))  # 文章\n",
    "dic_id2cat[int(item.Label)]  # id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4qiBmmdsJ-aK",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
    "batch_size = 16  # BERTでは16、32あたりを使用する\n",
    "\n",
    "dl_train = torchtext.data.Iterator(\n",
    "    dataset_train, batch_size=batch_size, train=True)\n",
    "\n",
    "dl_eval = torchtext.data.Iterator(\n",
    "    dataset_eval, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "dl_test = torchtext.data.Iterator(\n",
    "    dataset_test, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {\"train\": dl_train, \"val\": dl_eval}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "5-sNpiK5K14s",
    "outputId": "b2322c1e-977b-4728-82f0-1133716a035c",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[torchtext.data.batch.Batch of size 16]\n",
      "\t[.Text]:('[torch.LongTensor of size 16x512]', '[torch.LongTensor of size 16]')\n",
      "\t[.Label]:[torch.LongTensor of size 16]\n",
      "torch.Size([16, 512])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# DataLoaderの動作確認 \n",
    "\n",
    "batch = next(iter(dl_test))\n",
    "print(batch)\n",
    "print(batch.Text[0].shape)\n",
    "print(batch.Label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8e6NhQ3cLcq"
   },
   "source": [
    "## 準備3：BERTのクラス分類用のモデルを用意する\n",
    "\n",
    "Huggingfaceさんのをそのまま使うのではなく、BERTのbaseだけ使い、残りは自分で実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hFlvnI05a4xN",
    "outputId": "0c488a8d-1f1d-45c2-bfc8-f2b40e5e7fa2",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers.modeling_bert import BertModel\n",
    "\n",
    "# BERTの日本語学習済みパラメータのモデルです\n",
    "model = BertModel.from_pretrained('bert-base-japanese-whole-word-masking')\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BgGd7fLPssV",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class BertForLivedoor(nn.Module):\n",
    "    '''BERTモデルにLivedoorニュースの9クラスを判定する部分をつなげたモデル'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BertForLivedoor, self).__init__()\n",
    "\n",
    "        # BERTモジュール\n",
    "        self.bert = model  # 日本語学習済みのBERTモデル\n",
    "\n",
    "        # headにポジネガ予測を追加\n",
    "        # 入力はBERTの出力特徴量の次元768、出力は9クラス\n",
    "        self.cls = nn.Linear(in_features=768, out_features=9)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        '''\n",
    "\n",
    "        # BERTの基本モデル部分の順伝搬\n",
    "        # 順伝搬させる\n",
    "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
    "\n",
    "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
    "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
    "        vec_0 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素\n",
    "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_size]に変換\n",
    "        output = self.cls(vec_0)  # 全結合層\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IOtveynWRwK5",
    "outputId": "e142a63f-2d08-4614-ae02-b9221364c259",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワーク設定完了\n"
     ]
    }
   ],
   "source": [
    "# モデル構築\n",
    "net = BertForLivedoor()\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()\n",
    "\n",
    "print('ネットワーク設定完了')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hkg7r2RIR7qU"
   },
   "source": [
    "## 準備4：BERTのファインチューニングの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPGYvX4RR3UT",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
    "\n",
    "# 1. まず全部を、勾配計算Falseにしてしまう\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
    "for param in net.bert.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. 識別器を勾配計算ありに変更\n",
    "for param in net.cls.parameters():\n",
    "    param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Il_-wow4Suwe",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# BERTの元の部分はファインチューニング\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.cls.parameters(), 'lr': 1e-4}\n",
    "])\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zu7KRn1bTIQp"
   },
   "source": [
    "## 5. 訓練を実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZNaAXgiITFiw",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# モデルを学習させる関数を作成\n",
    "\n",
    "\n",
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "\n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.Text[0].to(device)  # 文章\n",
    "                labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # BERTに入力\n",
    "                    outputs = net(inputs)\n",
    "\n",
    "                    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            acc = (torch.sum(preds == labels.data)\n",
    "                                   ).double()/batch_size\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(\n",
    "                                iteration, loss.item(),  acc))\n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # epochごとのlossと正解率\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double(\n",
    "            ) / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
    "                                                                           phase, epoch_loss, epoch_acc))\n",
    "\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "JfnH-gAmS75e",
    "outputId": "e973a53b-f0ff-4b5c-f597-2afbef9f3837",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cuda:0\n",
      "-----start-------\n",
      "イテレーション 10 || Loss: 2.1159 || 10iter. || 本イテレーションの正解率：0.25\n",
      "イテレーション 20 || Loss: 1.9992 || 10iter. || 本イテレーションの正解率：0.375\n",
      "イテレーション 30 || Loss: 1.5570 || 10iter. || 本イテレーションの正解率：0.625\n",
      "イテレーション 40 || Loss: 1.3301 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 50 || Loss: 1.7788 || 10iter. || 本イテレーションの正解率：0.3125\n",
      "イテレーション 60 || Loss: 0.8538 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 70 || Loss: 0.9669 || 10iter. || 本イテレーションの正解率：0.625\n",
      "イテレーション 80 || Loss: 0.8314 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 90 || Loss: 0.4187 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 100 || Loss: 1.3798 || 10iter. || 本イテレーションの正解率：0.625\n",
      "イテレーション 110 || Loss: 0.7275 || 10iter. || 本イテレーションの正解率：0.6875\n",
      "イテレーション 120 || Loss: 0.3552 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 130 || Loss: 0.7527 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 140 || Loss: 0.7155 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 150 || Loss: 0.3241 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 160 || Loss: 0.3351 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 170 || Loss: 0.2045 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 180 || Loss: 0.2370 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 190 || Loss: 0.2049 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 200 || Loss: 0.9191 || 10iter. || 本イテレーションの正解率：0.625\n",
      "イテレーション 210 || Loss: 0.4397 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 220 || Loss: 0.3112 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 230 || Loss: 0.7040 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 240 || Loss: 0.4558 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 250 || Loss: 0.4454 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 260 || Loss: 0.4411 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 270 || Loss: 0.6963 || 10iter. || 本イテレーションの正解率：0.6875\n",
      "Epoch 1/4 | train |  Loss: 0.8414 Acc: 0.7345\n",
      "Epoch 1/4 |  val  |  Loss: 0.3906 Acc: 0.8631\n",
      "イテレーション 10 || Loss: 0.6791 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 20 || Loss: 0.3381 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 30 || Loss: 0.0783 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 40 || Loss: 0.2511 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 50 || Loss: 0.5420 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 60 || Loss: 0.6259 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 70 || Loss: 0.1505 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 80 || Loss: 0.4122 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 90 || Loss: 0.1113 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 100 || Loss: 0.0920 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 110 || Loss: 0.3323 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 120 || Loss: 0.5755 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 130 || Loss: 0.2081 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 140 || Loss: 0.2447 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 150 || Loss: 0.2027 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 160 || Loss: 0.4680 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 170 || Loss: 0.6421 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 180 || Loss: 0.3450 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 190 || Loss: 0.5139 || 10iter. || 本イテレーションの正解率：0.75\n",
      "イテレーション 200 || Loss: 0.4779 || 10iter. || 本イテレーションの正解率：0.6875\n",
      "イテレーション 210 || Loss: 0.1709 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 220 || Loss: 0.0931 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 230 || Loss: 0.0773 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 240 || Loss: 0.1433 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 250 || Loss: 0.2431 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 260 || Loss: 0.1826 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 270 || Loss: 0.1017 || 10iter. || 本イテレーションの正解率：1.0\n",
      "Epoch 2/4 | train |  Loss: 0.3196 Acc: 0.8983\n",
      "Epoch 2/4 |  val  |  Loss: 0.3453 Acc: 0.8902\n",
      "イテレーション 10 || Loss: 0.1392 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 20 || Loss: 0.1767 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 30 || Loss: 0.0272 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 40 || Loss: 0.0962 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 50 || Loss: 0.1329 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 60 || Loss: 0.3433 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 70 || Loss: 0.1884 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 80 || Loss: 0.5107 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 90 || Loss: 0.2070 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 100 || Loss: 0.3270 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 110 || Loss: 0.2003 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 120 || Loss: 0.2357 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 130 || Loss: 0.0670 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 140 || Loss: 0.0734 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 150 || Loss: 0.1997 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 160 || Loss: 0.3603 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 170 || Loss: 0.4984 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 180 || Loss: 0.2214 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 190 || Loss: 0.0837 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 200 || Loss: 0.3777 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 210 || Loss: 0.4441 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 220 || Loss: 0.1876 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 230 || Loss: 0.2209 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 240 || Loss: 0.0388 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 250 || Loss: 0.4409 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 260 || Loss: 0.1074 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 270 || Loss: 0.4357 || 10iter. || 本イテレーションの正解率：0.875\n",
      "Epoch 3/4 | train |  Loss: 0.2253 Acc: 0.9270\n",
      "Epoch 3/4 |  val  |  Loss: 0.2894 Acc: 0.9153\n",
      "イテレーション 10 || Loss: 0.1987 || 10iter. || 本イテレーションの正解率：0.875\n",
      "イテレーション 20 || Loss: 0.0759 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 30 || Loss: 0.0906 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 40 || Loss: 0.1452 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 50 || Loss: 0.1891 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 60 || Loss: 0.0669 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 70 || Loss: 0.2004 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 80 || Loss: 0.0190 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 90 || Loss: 0.0675 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 100 || Loss: 0.2610 || 10iter. || 本イテレーションの正解率：0.8125\n",
      "イテレーション 110 || Loss: 0.1047 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 120 || Loss: 0.0276 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 130 || Loss: 0.0369 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 140 || Loss: 0.0151 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 150 || Loss: 0.0441 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 160 || Loss: 0.0458 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 170 || Loss: 0.0536 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 180 || Loss: 0.2322 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 190 || Loss: 0.1565 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 200 || Loss: 0.1284 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 210 || Loss: 0.2975 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 220 || Loss: 0.0813 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 230 || Loss: 0.1267 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 240 || Loss: 0.0581 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 250 || Loss: 0.0579 || 10iter. || 本イテレーションの正解率：1.0\n",
      "イテレーション 260 || Loss: 0.2493 || 10iter. || 本イテレーションの正解率：0.9375\n",
      "イテレーション 270 || Loss: 0.0209 || 10iter. || 本イテレーションの正解率：1.0\n",
      "Epoch 4/4 | train |  Loss: 0.1562 Acc: 0.9489\n",
      "Epoch 4/4 |  val  |  Loss: 0.2879 Acc: 0.9139\n"
     ]
    }
   ],
   "source": [
    "# 学習・検証を実行する。1epochに2分ほどかかります\n",
    "num_epochs = 4\n",
    "net_trained = train_model(net, dataloaders_dict,\n",
    "                          criterion, optimizer, num_epochs=num_epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8zuZC_0yaOs"
   },
   "source": [
    "## テストデータでの性能を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "EdxKZzijT5PG",
    "outputId": "536411ea-c7e3-48c2-e60f-5c660127aea7",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93/93 [00:24<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テストデータ1475個での正解率：0.9261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# テストデータでの正解率を求める\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_trained.eval()   # モデルを検証モードに\n",
    "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
    "\n",
    "# epochの正解数を記録する変数\n",
    "epoch_corrects = 0\n",
    "\n",
    "for batch in tqdm(dl_test):  # testデータのDataLoader\n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = batch.Text[0].to(device)  # 文章\n",
    "    labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "    # 順伝搬（forward）計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # BertForLivedoorに入力\n",
    "        outputs = net_trained(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n",
    "\n",
    "# 正解率\n",
    "epoch_acc = epoch_corrects.double() / len(dl_test.dataset)\n",
    "\n",
    "print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), epoch_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XYvVlrl7y45g"
   },
   "source": [
    "https://yoheikikuta.github.io/bert-japanese/\n",
    "\n",
    "https://github.com/yoheikikuta/bert-japanese\n",
    "\n",
    "の「BERT with SentencePiece for Japanese text.」\n",
    "\n",
    "では、入力テキストにタイトルを含めていますが、今回はタイトルは除いています。\n",
    "\n",
    "同様にタイトルを抜いている、[BERTを用いた日本語文書分類タスクの学習・ハイパーパラメータチューニングの実践例](https://medium.com/karakuri/bert%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E6%97%A5%E6%9C%AC%E8%AA%9E%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E%E3%82%BF%E3%82%B9%E3%82%AF%E3%81%AE%E5%AD%A6%E7%BF%92-%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E5%AE%9F%E8%B7%B5%E4%BE%8B-2fa5e4299b16)でも、正解率が92%ちょっととなっており、ほぼ同じ正解率が得られました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Y37pHqN2YD0"
   },
   "source": [
    "以上。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2_BERT_livedoor_news_on_Google_Colaboratory.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
