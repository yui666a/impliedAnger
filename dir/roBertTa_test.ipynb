{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa8ec40-67e6-4883-8617-7a385378ca13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 14:29:16.180160: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-10 14:29:18.317592: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-10 14:29:18.317694: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-10 14:29:18.317705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "/home/nb-user/.local/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 参考文献\n",
    "# https://zenn.dev/schnell/articles/c5d9bb70490305\n",
    "\n",
    "# %%capture output\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "ja_models = [\"nlp-waseda/roberta-base-japanese\",\"nlp-waseda/roberta-large-japanese\",\"nlp-waseda/roberta-large-japanese-seq512\"]\n",
    "ja_model = ja_models[1]\n",
    "\n",
    "tokenizer =    AutoTokenizer.from_pretrained(ja_model)\n",
    "model = AutoModelForMaskedLM.from_pretrained(ja_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee410f99-a623-4f5e-bb28-31ccfbc3b289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本 語 の 自然 言語 処理 は 'こちら' 。\n",
      "日本 語 の 自然 言語 処理 は '難しい' 。\n",
      "日本 語 の 自然 言語 処理 は '可能です' 。\n",
      "日本 語 の 自然 言語 処理 は '困難である' 。\n",
      "日本 語 の 自然 言語 処理 は 'これ' 。\n"
     ]
    }
   ],
   "source": [
    "# maskされたトークン位置を取得する\n",
    "def get_masked_index(encoding):\n",
    "    for idx, id in enumerate(encoding.input_ids.squeeze(0)):\n",
    "        if id == tokenizer.mask_token_id:\n",
    "            return idx\n",
    "\n",
    "# 入力系列の処理\n",
    "sentence = '日本 語 の 自然 言語 処理 は [MASK] 。'\n",
    "encoding = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "# 予測\n",
    "pred = model(**encoding)\n",
    "\n",
    "# 後処理(取得した予測分布から、maskトークンの上位k件の予測結果を取得)\n",
    "masked_idx = get_masked_index(encoding)\n",
    "target_probs = pred.logits.squeeze(0)[masked_idx]\n",
    "\n",
    "# 上位k件の予測結果をのindexを取得\n",
    "top_k = torch.topk(target_probs, k=5).indices\n",
    "for id in top_k:\n",
    "    print(sentence.replace(\"[MASK]\", f\"'{tokenizer.decode(id)}'\"))\n",
    "# 日本 語 の 自然 言語 処理 は '得意' 。\n",
    "# 日本 語 の 自然 言語 処理 は 'できる' 。\n",
    "# 日本 語 の 自然 言語 処理 は '専門' 。\n",
    "# 日本 語 の 自然 言語 処理 は 'ない' 。\n",
    "# 日本 語 の 自然 言語 処理 は '苦手' 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67469380-63ce-4f61-b607-b8463915737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f73cd9aa-6a06-462e-9465-997dfdaecb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Name: tokenizers\n",
      "Version: 0.13.2\n",
      "Summary: Fast and Customizable Tokenizers\n",
      "Home-page: https://github.com/huggingface/tokenizers\n",
      "Author: Anthony MOI\n",
      "Author-email: anthony@huggingface.co\n",
      "License: Apache License 2.0\n",
      "Location: /home/nb-user/.local/lib/python3.8/site-packages\n",
      "Requires: \n",
      "Required-by: transformers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Name: transformers\n",
      "Version: 4.26.1\n",
      "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
      "Home-page: https://github.com/huggingface/transformers\n",
      "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
      "Author-email: transformers@huggingface.co\n",
      "License: Apache\n",
      "Location: /home/nb-user/.local/lib/python3.8/site-packages\n",
      "Requires: pyyaml, huggingface-hub, tqdm, requests, packaging, filelock, tokenizers, regex, numpy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# !pip show tokenizers\n",
    "!pip show transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
