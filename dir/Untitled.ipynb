{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1aa1e6-e33c-4025-9bd1-26a30f08f917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,DistilBertTokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import RobertaTokenizer,DebertaV2Tokenizer\n",
    "# MDL_PATH=\"microsoft/deberta-v3-base\"\n",
    "MDL_PATH=\"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MDL_PATH)\n",
    "# テキストのリストを専用の入力データに変換\n",
    "def to_features(texts, max_length):\n",
    "    shape = (len(texts), max_length)\n",
    "    # input_idsやattention_mask, token_type_ids\n",
    "    input_ids = np.zeros(shape, dtype=\"int32\")\n",
    "    attention_mask = np.zeros(shape, dtype=\"int32\")\n",
    "    for i, text in enumerate(texts):\n",
    "        encoded_dict = tokenizer.encode_plus(text, max_length=max_length, pad_to_max_length=True,truncation=True)\n",
    "        input_ids[i] = encoded_dict[\"input_ids\"]\n",
    "        attention_mask[i] = encoded_dict[\"attention_mask\"]\n",
    "    return [tf.cast(input_ids, tf.int32), tf.cast(attention_mask, tf.int32)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0b76caa-41c5-4f25-ab50-8e8ed1b7d963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1dc59-4a45-499b-8bc2-8ef2de53107c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データサイズ： (1800, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fulltext</th>\n",
       "      <th>sentences</th>\n",
       "      <th>danwa_result</th>\n",
       "      <th>result_form</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>一生懸命探したが、開け口が見つからず開封に苦労しました。開け易いよう切り込み付けて欲しいです。</td>\n",
       "      <td>[一生懸命探したが、開け口が見つからず開封に苦労しました, 開け易いよう切り込み付けて欲しいです]</td>\n",
       "      <td>要望</td>\n",
       "      <td>{'2': '7.0', '5': '1.0', '1': '2.0'}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>APの回復が遅すぎる。キャラを育てにくい</td>\n",
       "      <td>[APの回復が遅すぎる, キャラを育てにくい]</td>\n",
       "      <td>並列</td>\n",
       "      <td>{'3': '1.0', '0': '3.0', '2': '2.0', '1': '4.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>6シリーズがあり、M3があり、敢えて4シリーズをラインナップさせているのがよくわからない!</td>\n",
       "      <td>[6シリーズがあり、M3があり、敢えて4シリーズをラインナップさせているのがよくわからない!]</td>\n",
       "      <td>順接</td>\n",
       "      <td>{'3': '2.0', '2': '1.0', '0': '2.0', '5': '1.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             fulltext  \\\n",
       "981   一生懸命探したが、開け口が見つからず開封に苦労しました。開け易いよう切り込み付けて欲しいです。   \n",
       "764                              APの回復が遅すぎる。キャラを育てにくい   \n",
       "1054    6シリーズがあり、M3があり、敢えて4シリーズをラインナップさせているのがよくわからない!   \n",
       "\n",
       "                                              sentences danwa_result  \\\n",
       "981   [一生懸命探したが、開け口が見つからず開封に苦労しました, 開け易いよう切り込み付けて欲しいです]           要望   \n",
       "764                             [APの回復が遅すぎる, キャラを育てにくい]           並列   \n",
       "1054    [6シリーズがあり、M3があり、敢えて4シリーズをラインナップさせているのがよくわからない!]           順接   \n",
       "\n",
       "                                            result_form  label  \n",
       "981                {'2': '7.0', '5': '1.0', '1': '2.0'}      2  \n",
       "764   {'3': '1.0', '0': '3.0', '2': '2.0', '1': '4.0...      1  \n",
       "1054  {'3': '2.0', '2': '1.0', '0': '2.0', '5': '1.0...      1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"ksl_pub/suggestive_anger/data/fuman_survey/json/human_data_ver5.json\")\n",
    "print(f'データサイズ： {dataset.shape}')\n",
    "\n",
    "# for i in range(len(dataset.fulltext.values)):\n",
    "#     print(dataset.label.values[i],  dataset.fulltext.values[i])\n",
    "\n",
    "# データの抽出\n",
    "sentences = dataset.fulltext.values\n",
    "labels = dataset.label.values\n",
    "\n",
    "dataset.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e745fe13-903b-4905-81e4-48c533da0792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "#訓練用サイズを指定する場合(80%)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dataset[\"fulltext\"], dataset[\"label\"],train_size=0.8)\n",
    "\n",
    "\n",
    "# from torch.utils.data import TensorDataset, random_split\n",
    "# from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# # データセットクラスの作成\n",
    "# dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# # 90%地点のIDを取得\n",
    "# train_size = int(0.9 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "\n",
    "# # データセットを分割\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# print('訓練データ数：{}'.format(train_size))\n",
    "# print('検証データ数:　{} '.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "106d13b9-762c-4268-ac09-fdf5a20a8485",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nb-user/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[0;32m----> 2\u001b[0m x_train \u001b[38;5;241m=\u001b[39m \u001b[43mto_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m y_train\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mcast(train_labels, tf\u001b[38;5;241m.\u001b[39mint32)\n\u001b[1;32m      4\u001b[0m x_valid \u001b[38;5;241m=\u001b[39m to_features(valid_texts, max_length)\n",
      "Cell \u001b[0;32mIn[2], line 18\u001b[0m, in \u001b[0;36mto_features\u001b[0;34m(texts, max_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m     input_ids[i] \u001b[38;5;241m=\u001b[39m encoded_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     17\u001b[0m     attention_mask[i] \u001b[38;5;241m=\u001b[39m encoded_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39mcast(input_ids, tf\u001b[38;5;241m.\u001b[39mint32), tf\u001b[38;5;241m.\u001b[39mcast(attention_mask, tf\u001b[38;5;241m.\u001b[39mint32)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "max_length=500\n",
    "x_train = to_features(X_train, max_length)\n",
    "y_train=tf.cast(train_labels, tf.int32)\n",
    "x_valid = to_features(valid_texts, max_length)\n",
    "y_valid=tf.cast(valid_labels, tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfe5331-1302-47bd-9bf3-dcb5cd9eb265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
