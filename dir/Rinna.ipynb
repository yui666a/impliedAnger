{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1679d96-7b35-40d9-8e88-49f8287a436b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# GPUが使えれば利用する設定\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3115cba-9331-4840-8917-21539c8f8647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import RobertaConfig, RobertaModel\n",
    "# configuration = RobertaConfig()\n",
    "# model = RobertaModel(configuration)\n",
    "# configuration = model.config\n",
    "\n",
    "from transformers import T5Tokenizer, RobertaForMaskedLM\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-roberta-base\")\n",
    "tokenizer.do_lower_case = True  # due to some bug of tokenizer config loading\n",
    "\n",
    "model = RobertaForMaskedLM.from_pretrained(\"rinna/japanese-roberta-base\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48fc10d-f40d-4016-963c-66b7219ff8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # original text\n",
    "# text = \"ヤクルトの舌や喉に何か残る感じの、あれは何でしょう?すごくいやだ。\"\n",
    "\n",
    "# # prepend [CLS]\n",
    "# text = \"[CLS]\" + text\n",
    "\n",
    "# # tokenize\n",
    "# tokens = tokenizer.tokenize(text)\n",
    "# print(tokens)  # output: ['[CLS]', '▁4', '年に', '1', '度', 'オリンピック', 'は', '開かれる', '。']\n",
    "\n",
    "# # mask a token\n",
    "# masked_idx = 5\n",
    "# tokens[masked_idx] = tokenizer.mask_token\n",
    "# tokens[7] = tokenizer.mask_token\n",
    "# print(tokens)  # output: ['[CLS]', '▁4', '年に', '1', '度', '[MASK]', 'は', '開かれる', '。']\n",
    "\n",
    "# # convert to ids\n",
    "# token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# print(token_ids)  # output: [4, 1602, 44, 24, 368, 6, 11, 21583, 8]\n",
    "\n",
    "# # convert to tensor\n",
    "# import torch\n",
    "# token_tensor = torch.LongTensor([token_ids])\n",
    "\n",
    "# # provide position ids explicitly\n",
    "# position_ids = list(range(0, token_tensor.size(1)))\n",
    "# print(position_ids)  # output: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "# position_id_tensor = torch.LongTensor([position_ids])\n",
    "\n",
    "# # get the top 10 predictions of the masked token\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(input_ids=token_tensor, position_ids=position_id_tensor)\n",
    "#     predictions = outputs[0][0, masked_idx].topk(10)\n",
    "\n",
    "# for i, index_t in enumerate(predictions.indices):\n",
    "#     index = index_t.item()\n",
    "#     token = tokenizer.convert_ids_to_tokens([index])[0]\n",
    "#     print(i, token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dc87bf-9211-4174-97de-58d2c2113138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original text\n",
    "text = \"ヤクルトの舌や喉に何か残る感じの、あれは何でしょう?すごくいやだ。\"\n",
    "\n",
    "# prepend [CLS]\n",
    "text = \"[CLS]\" + text\n",
    "\n",
    "# tokenize\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)  # output: ['[CLS]', '▁4', '年に', '1', '度', 'オリンピック', 'は', '開かれる', '。']\n",
    "\n",
    "\n",
    "# マスク化される文字を一時的に保存する\n",
    "tmp = \"\"\n",
    "for i in range(2, len(tokens)):\n",
    "    # mask a token\n",
    "    masked_idx = i\n",
    "    tmp = tokens[masked_idx]\n",
    "    tokens[masked_idx] = tokenizer.mask_token\n",
    "    print(tokens)  # output: ['[CLS]', '▁4', '年に', '1', '度', '[MASK]', 'は', '開かれる', '。']\n",
    "\n",
    "    # convert to ids\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    print(token_ids)  # output: [4, 1602, 44, 24, 368, 6, 11, 21583, 8]\n",
    "\n",
    "    # convert to tensor\n",
    "    import torch\n",
    "    token_tensor = torch.LongTensor([token_ids])\n",
    "\n",
    "    # provide position ids explicitly\n",
    "    position_ids = list(range(0, token_tensor.size(1)))\n",
    "    print(position_ids)  # output: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "    position_id_tensor = torch.LongTensor([position_ids])\n",
    "\n",
    "    # get the top 10 predictions of the masked token\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=token_tensor, position_ids=position_id_tensor)\n",
    "        predictions = outputs[0][0, masked_idx].topk(10)\n",
    "\n",
    "    for i, index_t in enumerate(predictions.indices):\n",
    "        index = index_t.item()\n",
    "        token = tokenizer.convert_ids_to_tokens([index])[0]\n",
    "        print(i, token)\n",
    "    \n",
    "    # mask　から 通常の文字に戻す\n",
    "    tokens[masked_idx] = tmp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da8908a-53be-47e3-af67-009bb102f6af",
   "metadata": {},
   "source": [
    "# 暗示的怒り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaeb3380-839b-472d-bf37-aaeb630ac0e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "print(os.environ['CUDA_LAUNCH_BLOCKING'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2b238a-1d8a-4899-a7db-50567350ced6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "# GPUが使えれば利用する設定\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "668d36a5-9c3d-4366-8905-6d1d2d08f6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データサイズ： (1800, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fulltext</th>\n",
       "      <th>sentences</th>\n",
       "      <th>danwa_result</th>\n",
       "      <th>result_form</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>チェキカメラ、内カメラでもとれるようにしてほしい</td>\n",
       "      <td>[チェキカメラ、内カメラでもとれるようにしてほしい]</td>\n",
       "      <td>短文</td>\n",
       "      <td>{'4': '1.0', '5': '2.0', '2': '6.0', '1': '3.0...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1721</th>\n",
       "      <td>カチャカチャと音がするものしかない。無音のものがあるといいのに。</td>\n",
       "      <td>[カチャカチャと音がするものしかない, 無音のものがあるといいのに]</td>\n",
       "      <td>要望</td>\n",
       "      <td>{'5': '1.0', '4': '1.0', '2': '1.0', '1': '5.0...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>最近の雑誌は高い中身の内容薄いのに千円近くするおまけもいらない</td>\n",
       "      <td>[最近の雑誌は高い中身の内容薄いのに千円近くするおまけもいらない]</td>\n",
       "      <td>順接</td>\n",
       "      <td>{'4': '2.0', '0': '6.0', '1': '3.0', '2': '2.0'}</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              fulltext                           sentences  \\\n",
       "14            チェキカメラ、内カメラでもとれるようにしてほしい          [チェキカメラ、内カメラでもとれるようにしてほしい]   \n",
       "1721  カチャカチャと音がするものしかない。無音のものがあるといいのに。  [カチャカチャと音がするものしかない, 無音のものがあるといいのに]   \n",
       "19     最近の雑誌は高い中身の内容薄いのに千円近くするおまけもいらない   [最近の雑誌は高い中身の内容薄いのに千円近くするおまけもいらない]   \n",
       "\n",
       "     danwa_result                                        result_form  label  \n",
       "14             短文  {'4': '1.0', '5': '2.0', '2': '6.0', '1': '3.0...      2  \n",
       "1721           要望  {'5': '1.0', '4': '1.0', '2': '1.0', '1': '5.0...      1  \n",
       "19             順接   {'4': '2.0', '0': '6.0', '1': '3.0', '2': '2.0'}      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dataset = pd.read_json(\"ksl_pub/suggestive_anger/data/fuman_survey/json/human_data_ver5.json\")\n",
    "print(f'データサイズ： {dataset.shape}')\n",
    "\n",
    "# for i in range(len(dataset.fulltext.values)):\n",
    "#     print(dataset.label.values[i],  dataset.fulltext.values[i])\n",
    "\n",
    "# データの抽出\n",
    "sentences = dataset.fulltext.values\n",
    "labels = dataset.label.values\n",
    "\n",
    "dataset.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35f225d8-4c51-4b9f-a2b1-8a337f3e7c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-10 17:55:50.847849: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-10 17:55:52.951002: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-10 17:55:52.951096: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-02-10 17:55:52.951106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# 1. BERT Tokenizerを用いて単語分割・IDへ変換\n",
    "## Tokenizerの準備\n",
    "from transformers import BertJapaneseTokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "# from transformers import BertJapaneseTokenizer\n",
    "# tokenizer = BertJapaneseTokenizer.from_pretrained('rinna/japanese-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef1a6e70-7617-4a5e-99aa-49922eda68e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  最近の内容は、のび太が、他力本願ですぐ調子にのり、エゴが強すぎると感じている。子供に見せたくないアニメになった。\n",
      "Tokenized:  ['最近', 'の', '内容', 'は', '、', 'のび太', 'が', '、', '他', '##力', '##本', '##願', 'で', 'すぐ', '調子', 'に', 'のり', '、', 'エ', '##ゴ', 'が', '強', 'すぎる', 'と', '感じ', 'て', 'いる', '。', '子供', 'に', '見せ', 'たく', 'ない', 'アニメ', 'に', 'なっ', 'た', '。']\n",
      "Token IDs:  [5233, 5, 1386, 9, 6, 15217, 14, 6, 375, 28677, 28515, 29645, 12, 2459, 12327, 7, 6317, 6, 133, 28796, 14, 427, 8896, 13, 3415, 16, 33, 8, 1803, 7, 2685, 5034, 80, 1149, 7, 58, 10, 8]\n",
      "\n",
      "\n",
      "\n",
      "Original:  ヤクルトの舌や喉に何か残る感じの、あれは何でしょう?すごくいやだ。\n",
      "Tokenized:  ['ヤクルト', 'の', '舌', 'や', '喉', 'に', '何', 'か', '残る', '感じ', 'の', '、', 'あれ', 'は', '何', 'でしょ', 'う', '?', 'すご', '##く', 'いや', 'だ', '。']\n",
      "Token IDs:  [11489, 5, 10694, 49, 14355, 7, 1037, 29, 4105, 3415, 5, 6, 2787, 9, 1037, 11655, 205, 2935, 14993, 28504, 17989, 75, 8]\n",
      "\n",
      "\n",
      "\n",
      "Original:  めばちこ痛い。治ってもまたできる...\n",
      "Tokenized:  ['め', '##ば', '##ち', 'こ', '痛', '##い', '。', '治', '##っ', 'て', 'も', 'また', 'できる', '...']\n",
      "Token IDs:  [2087, 28620, 28586, 27, 4897, 28457, 8, 1311, 28468, 16, 28, 106, 392, 3215]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## テスト実行\n",
    "# 元文章\n",
    "for i in range(3):\n",
    "    print('Original: ', sentences[i])\n",
    "    print('Tokenized: ', tokenizer.tokenize(sentences[i]))    # Tokenizer\n",
    "    print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[i])))    # Token-id\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da5bb06e-7416-4d5e-806f-e0ff89521d87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大単語数:  185\n",
      "上記の最大単語数にSpecial token（[CLS], [SEP]）の+2をした値(187)が最大単語数\n"
     ]
    }
   ],
   "source": [
    "# 最大単語数の確認\n",
    "max_len = []\n",
    "# 1文づつ処理\n",
    "for sent in sentences:\n",
    "    # Tokenizeで分割\n",
    "    token_words = tokenizer.tokenize(sent)\n",
    "    # 文章数を取得してリストへ格納\n",
    "    max_len.append(len(token_words))\n",
    "# 最大の値を確認\n",
    "print('最大単語数: ', max(max_len))\n",
    "print('上記の最大単語数にSpecial token（[CLS], [SEP]）の+2をした値(' + str(max(max_len) + 2) + ')が最大単語数')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e71d063a-be0f-47a9-b4de-4bac9f0ea409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/nb-user/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  最近の内容は、のび太が、他力本願ですぐ調子にのり、エゴが強すぎると感じている。子供に見せたくないアニメになった。\n",
      "Token IDs: tensor([    2,  5233,     5,  1386,     9,     6, 15217,    14,     6,   375,\n",
      "        28677, 28515, 29645,    12,  2459, 12327,     7,  6317,     6,   133,\n",
      "        28796,    14,   427,  8896,    13,  3415,    16,    33,     8,  1803,\n",
      "            7,  2685,  5034,    80,  1149,     7,    58,    10,     8,     3,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# 1文づつ処理\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True, # Special Tokenの追加\n",
    "                        # max_length = 187,           # 文章の長さを固定（Padding/Trancatinating）\n",
    "                        max_length = max(max_len) + 2,           # 文章の長さを固定（Padding/Trancatinating）\n",
    "                        pad_to_max_length = True,# PADDINGで埋める\n",
    "                        return_attention_mask = True,   # Attention maksの作成\n",
    "                        return_tensors = 'pt',     #  Pytorch tensorsで返す\n",
    "                   )\n",
    "\n",
    "    # 単語IDを取得    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # Attention　maskの取得\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# リストに入ったtensorを縦方向（dim=0）へ結合\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# tenosor型に変換\n",
    "labels = torch.tensor(labels, device=device)\n",
    "\n",
    "# 確認\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "253f84ff-91c0-48b5-a075-7494d177b8a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練データ数：1620\n",
      "検証データ数:　180 \n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# データセットクラスの作成\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# 90%地点のIDを取得\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# データセットを分割\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('訓練データ数：{}'.format(train_size))\n",
    "print('検証データ数:　{} '.format(val_size))\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 32\n",
    "\n",
    "# 訓練データローダー\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset), # ランダムにデータを取得してバッチ化\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "# 検証データローダー\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), # 順番にデータを取得してバッチ化\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b140244-48ec-4f62-ac0d-10720a1324b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# BertForSequenceClassification 学習済みモデルのロード\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"cl-tohoku/bert-base-japanese-whole-word-masking\", # 日本語Pre trainedモデルの指定\n",
    "    num_labels = 6, # ラベル数\n",
    "    output_attentions = False, # アテンションベクトルを出力するか\n",
    "    output_hidden_states = False, # 隠れ層を出力するか\n",
    ").to(device)\n",
    "\n",
    "# モデルをGPUへ転送\n",
    "# 下の方でエラーが発生するため、一時的に無効化\n",
    "model.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e828b411-5645-4dc8-b9dc-a09c8c349410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 最適化手法の設定\n",
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# # 訓練パートの定義\n",
    "# def train(model):\n",
    "#     model.train() # 訓練モードで実行\n",
    "#     train_loss = 0\n",
    "#     for batch in train_dataloader:# train_dataloaderはword_id, mask, labelを出力する点に注意\n",
    "#         b_input_ids = batch[0].to(device)\n",
    "#         b_input_mask = batch[1].to(device)\n",
    "#         b_labels = batch[2].to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss, logits = model(b_input_ids, \n",
    "#                              token_type_ids=None, \n",
    "#                              attention_mask=b_input_mask, \n",
    "#                              labels=b_labels)\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         optimizer.step()\n",
    "#         train_loss += loss.item()\n",
    "#     return train_loss\n",
    "\n",
    "# # テストパートの定義\n",
    "# def validation(model):\n",
    "#     model.eval()# 訓練モードをオフ\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad(): # 勾配を計算しない\n",
    "#         for batch in validation_dataloader:\n",
    "#             b_input_ids = batch[0].to(device)\n",
    "#             b_input_mask = batch[1].to(device)\n",
    "#             b_labels = batch[2].to(device)\n",
    "#             with torch.no_grad():        \n",
    "#                 (loss, logits) = model(b_input_ids, \n",
    "#                                     token_type_ids=None, \n",
    "#                                     attention_mask=b_input_mask,\n",
    "#                                     labels=b_labels)\n",
    "#             val_loss += loss.item()\n",
    "#     return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2662ef60-ec32-4bb9-95cf-ac1ac66e6621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 訓練パートの定義\n",
    "def train(model):\n",
    "    model.train() # 訓練モードで実行\n",
    "    train_loss = 0\n",
    "    for batch in train_dataloader: # train_dataloaderはword_id, mask, labelを出力する点に注意\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask, \n",
    "                            labels=b_labels).loss # 戻り値とここを修正\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss\n",
    "\n",
    "# テストパートの定義\n",
    "def validation(model):\n",
    "    model.eval() # 訓練モードをオフ\n",
    "    val_loss = 0\n",
    "    with torch.no_grad(): # 勾配を計算しない\n",
    "        for batch in validation_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            with torch.no_grad():        \n",
    "                loss = model(b_input_ids, \n",
    "                                    token_type_ids=None, \n",
    "                                    attention_mask=b_input_mask,\n",
    "                                    labels=b_labels).loss # 戻り値とここを修正\n",
    "            val_loss += loss.item()\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3aeb206-a022-41a0-ac76-10dae7163e17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[74.00467312335968]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 学習の実行\n",
    "max_epoch = 1\n",
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    train_ = train(model)\n",
    "    test_ = train(model)\n",
    "    train_loss_.append(train_)\n",
    "    test_loss_.append(test_)\n",
    "train_loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d01ddce-4b36-4085-ae01-b6b3e3bdf57d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 検証方法の確認（1バッチ分で計算ロジックに確認）\n",
    "\n",
    "model.eval()# 訓練モードをオフ\n",
    "for batch in validation_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    with torch.no_grad():   \n",
    "        # 学習済みモデルによる予測結果をpredsで取得     \n",
    "        preds = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7b3b532-62a5-40d9-a8be-523aa8ce045e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "出力:SequenceClassifierOutput(loss=None, logits=tensor([[-0.1078,  2.3627, -1.7634, -0.3342, -0.5414, -0.6090],\n",
      "        [ 0.6518,  0.3105, -1.8580,  0.0424, -0.5819,  0.5271],\n",
      "        [-1.3868,  0.8976, -0.2736, -0.9676,  0.3157, -0.4518],\n",
      "        [-0.2521,  2.3922, -1.6009, -0.3352, -0.3784, -0.8180],\n",
      "        [ 0.6905,  0.6604, -1.8571,  0.1842, -0.5850,  0.2643],\n",
      "        [ 0.6086,  1.1970, -1.9666,  0.2025, -0.6530, -0.0635],\n",
      "        [ 0.5671,  1.1594, -1.4935, -0.3725, -0.6269,  0.1761],\n",
      "        [-1.8114,  0.0439,  3.3915, -0.2929, -1.1834, -1.1395],\n",
      "        [-0.5611,  0.5423, -1.7004, -0.2798,  0.2394,  0.0158],\n",
      "        [-1.4774,  0.6904, -0.4987, -0.7212,  0.5240, -0.3153],\n",
      "        [-1.6062,  0.4561,  3.7199, -0.8118, -1.1322, -1.0862],\n",
      "        [-1.5611,  0.0809,  3.0536, -0.1876, -0.9983, -1.2292],\n",
      "        [-0.9971,  1.2709, -0.3272, -0.2107,  0.1839, -0.8773],\n",
      "        [-1.5705, -0.0751,  3.1134, -0.2249, -1.0056, -1.1878],\n",
      "        [-1.4181,  1.9073,  0.8834, -0.8319, -0.8379, -0.6335],\n",
      "        [ 0.2440,  2.0392, -1.8900,  0.2624, -0.5091, -0.7920],\n",
      "        [-1.4624,  0.9003,  3.1762, -0.7290, -1.5025, -1.2553],\n",
      "        [-1.1956,  0.6434,  2.0350,  0.5029, -1.4351, -1.0605],\n",
      "        [-1.9701,  1.4571,  1.6241, -0.3487, -0.5804, -1.1376],\n",
      "        [-0.5391,  0.4529, -1.4415, -0.3606,  0.3247,  0.3991]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "## 予測結果の確認\n",
    "print(f'出力:{preds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a34de037-255c-4239-8adb-9558f0f79ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>感情的攻撃</th>\n",
       "      <th>感情的説得</th>\n",
       "      <th>理性的説得</th>\n",
       "      <th>嫌味・皮肉</th>\n",
       "      <th>遠回し</th>\n",
       "      <th>怒ってない</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>true_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.107773</td>\n",
       "      <td>2.362728</td>\n",
       "      <td>-1.763384</td>\n",
       "      <td>-0.334184</td>\n",
       "      <td>-0.541413</td>\n",
       "      <td>-0.608966</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.651800</td>\n",
       "      <td>0.310498</td>\n",
       "      <td>-1.857996</td>\n",
       "      <td>0.042417</td>\n",
       "      <td>-0.581905</td>\n",
       "      <td>0.527077</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.386837</td>\n",
       "      <td>0.897578</td>\n",
       "      <td>-0.273604</td>\n",
       "      <td>-0.967604</td>\n",
       "      <td>0.315678</td>\n",
       "      <td>-0.451812</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.252058</td>\n",
       "      <td>2.392193</td>\n",
       "      <td>-1.600874</td>\n",
       "      <td>-0.335171</td>\n",
       "      <td>-0.378388</td>\n",
       "      <td>-0.818017</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.690539</td>\n",
       "      <td>0.660420</td>\n",
       "      <td>-1.857126</td>\n",
       "      <td>0.184235</td>\n",
       "      <td>-0.585034</td>\n",
       "      <td>0.264289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      感情的攻撃     感情的説得     理性的説得     嫌味・皮肉       遠回し     怒ってない  pred_label  \\\n",
       "0 -0.107773  2.362728 -1.763384 -0.334184 -0.541413 -0.608966           1   \n",
       "1  0.651800  0.310498 -1.857996  0.042417 -0.581905  0.527077           0   \n",
       "2 -1.386837  0.897578 -0.273604 -0.967604  0.315678 -0.451812           1   \n",
       "3 -0.252058  2.392193 -1.600874 -0.335171 -0.378388 -0.818017           1   \n",
       "4  0.690539  0.660420 -1.857126  0.184235 -0.585034  0.264289           0   \n",
       "\n",
       "   true_label  \n",
       "0           1  \n",
       "1           3  \n",
       "2           1  \n",
       "3           1  \n",
       "4           0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比較しやすい様にpd.dataframeへ整形\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# pd.dataframeへ変換（GPUに乗っているTensorはgpu->cpu->numpy->dataframeと変換）\n",
    "logits_df = pd.DataFrame(preds[0].cpu().numpy(), columns=['感情的攻撃', '感情的説得', '理性的説得', '嫌味・皮肉', '遠回し', '怒ってない'])\n",
    "## np.argmaxで大き方の値を取得\n",
    "pred_df = pd.DataFrame(np.argmax(preds[0].cpu().numpy(), axis=1), columns=['pred_label'])\n",
    "label_df = pd.DataFrame(b_labels.cpu().numpy(), columns=['true_label'])\n",
    "\n",
    "accuracy_df = pd.concat([logits_df, pred_df, label_df], axis=1)\n",
    "\n",
    "accuracy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c2374-6887-450b-88bb-7d6c17024512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name())\n",
    "# > Tesla T4\n",
    "print(torch.cuda.get_device_capability())\n",
    "# > (7, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e627e6-7782-4703-b778-bc9755b48dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# 変数を用意\n",
    "a = torch.randn(5)\n",
    "b = torch.randn(5)\n",
    "\n",
    "# 今回は変数aだけをGPUに送る。\n",
    "a = a.to(device)\n",
    "b = b.to(device)\n",
    "\n",
    "print(a + b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
